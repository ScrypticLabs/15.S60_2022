{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy Space Method: Cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ScrypticLabs/15.S60_2022/blob/main/Policy_Space_Method_Cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup \n",
        "The following code sets up requirements, imports, and helper functions. Please do not modify this section of code."
      ],
      "metadata": {
        "id": "xiM6BzNCSP3R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwJ-PjDy0Doe"
      },
      "outputs": [],
      "source": [
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!sudo apt-get update > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "%matplotlib inline\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from tqdm.notebook import trange\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'setup_display' in locals():\n",
        "    raise RuntimeError(\"Don't run this cell twice!\")\n",
        "\n",
        "setup_display = True\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=False, size=(1400, 900))\n",
        "display.start()"
      ],
      "metadata": {
        "id": "Mon_amE80h5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "367b06dc-671e-451a-da85-595452cb965b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f5fb30e44d0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\n",
        "Adapt from https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t#scrollTo=8nj5sjsk15IT\n",
        "\"\"\"\n",
        "def show_video(directory='video'):\n",
        "    mp4list = glob.glob(f'{directory}/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "      mp4 = mp4list[0]\n",
        "      video = io.open(mp4, 'r+b').read()\n",
        "      encoded = base64.b64encode(video)\n",
        "      ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                  loop controls style=\"height: 200px;\">\n",
        "                  <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "              </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "      print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env, directory='./video'):\n",
        "    env = Monitor(env, directory, force=True)\n",
        "    return env"
      ],
      "metadata": {
        "id": "m5L29DZN0koK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility functions to load and save torch model checkpoints \n",
        "\"\"\"\n",
        "def load_checkpoint(net, optimizer=None, step='max', save_dir='checkpoints'):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    checkpoints = [x for x in os.listdir(save_dir) if not x.startswith('events')]\n",
        "    if step == 'max':\n",
        "        step = 0\n",
        "        if checkpoints:\n",
        "            step, last_checkpoint = max([(int(x.split('.')[0]), x) for x in checkpoints])\n",
        "    else:\n",
        "        last_checkpoint = str(step) + '.pth'\n",
        "    if step:\n",
        "        save_path = os.path.join(save_dir, last_checkpoint)\n",
        "        state = torch.load(save_path, map_location='cpu')\n",
        "        net.load_state_dict(state['net'])\n",
        "        if optimizer:\n",
        "            optimizer.load_state_dict(state['optimizer'])\n",
        "        print('Loaded checkpoint %s' % save_path)\n",
        "    return step\n",
        "\n",
        "def save_checkpoint(net, optimizer, step, save_dir='checkpoints'):\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    save_path = os.path.join(save_dir, str(step) + '.pth')\n",
        "\n",
        "    torch.save(dict(net=net.state_dict(), optimizer=optimizer.state_dict()), save_path)\n",
        "    print('Saved checkpoint %s' % save_path)"
      ],
      "metadata": {
        "id": "II72pkFJSd1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gym Environment\n",
        "We will work with environment definitions from the OpenAI Gym library. Gym consists of several categories of environments (see https://gym.openai.com/ for some graphical examples):\n",
        "- Classical control tasks: CartPole, MountainCar, etc. These tasks are relatively simple tasks which are sometimes also studied in control theory. They usually serve as diagnostic tasks for RL algorithms.\n",
        "- Atari tasks: Breakout, Pong, SpaceInvader, etc. These tasks are based on games that ran on the [Atari System](https://en.wikipedia.org/wiki/Atari_2600). These tasks were popularized by DeepMind in the original DQN paper (linked above) and all have discrete action spaces, which is why a lot of DQN-based methods use these tasks as benchmarks.\n",
        "- MuJoCo, Box2d, and other tasks. We will not discuss these tasks in this homework, but some are commonly used for DQN-based methods while other are used for policy-gradient-based methods.\n",
        "\n",
        "We will work on the classical control task CartPole this time."
      ],
      "metadata": {
        "id": "MZir-drn1CjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The below two cells demonstate interacting with Gym environments with uniformly randomly selected actions. Please familiarize with the method and attributes of the environment instance `env` that we use in these cells, in particular notice `env.reset()`, `env.step()`, `env.render()`, `env.observation_space`, and `env.action_space`."
      ],
      "metadata": {
        "id": "RNk_cinBSe0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task= 'CartPole-v1'\n",
        "video_dir = f'{task}_vis'\n",
        "env = wrap_env(gym.make(task), video_dir)\n",
        "\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "    env.render()\n",
        "    \n",
        "    # Sample a random action from the action space\n",
        "    action = env.action_space.sample() \n",
        "    \n",
        "    # Take a step in the environment with the action\n",
        "    observation, reward, done, info = env.step(action) \n",
        "       \n",
        "    if done:  # If the episode is done\n",
        "      break\n",
        "            \n",
        "env.close()\n",
        "show_video(video_dir)"
      ],
      "metadata": {
        "id": "VyMkHNQx0qoZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "10cd7c44-f139-4e85-9ceb-b7d447aa81d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                  loop controls style=\"height: 200px;\">\n",
              "                  <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "              </video>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous spaces (such as observation space, which we called $\\mathcal{S}$ in class) are defined by the `gym.Box` class; for example `env.observation_space` specifies the Numpy array shape (`env.observation_space.shape`) of each observation. Discrete spaces are defined by the `gym.Discrete` class; for example `env.action_space` represents the set `{0, 1, ..., env.action_space.n - 1}`, which we called $\\mathcal{A}$ in class. "
      ],
      "metadata": {
        "id": "tBI--6O4S2Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Observation (state) space:', env.observation_space)\n",
        "print('Action space:', env.action_space)"
      ],
      "metadata": {
        "id": "L44Glt_6S4LN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a883878-e717-44f4-f996-68670544788c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation (state) space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "Action space: Discrete(2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Policy Gradient for Cartpole\n",
        "We will implement the REINFORCE algorithm and some variants with PyTorch, a popular automatic differentiation framework. If you're not familiar with PyTorch, please find a tutorial online if necessary (e.g. [here](https://pytorch.org/tutorials/)).\n",
        "<!-- \n",
        "**General comment: you do not need to strictly follow the method signature as defined in the starter code that we provide. We provide detailed types for the arguments and return values just as optional advice. You may modify the arguments as you see fit, as long as the overall structure is similar.** -->"
      ],
      "metadata": {
        "id": "s742zpuyS73j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be experimenting with\n",
        "\n",
        "- Vanilla v.s. temporal structure\n",
        "- No baseline v.s. a value function baseline\n",
        "\n",
        "using a neural network with fully connected layers, which has input tensor size corresponding to the observation_space.shape, two intermediate layers each with size 64, and output size of action_space.n. As a reminder, the REINFORCE objective for the vanilla version is\n",
        "$$V(\\pi_\\theta) = \\mathbb{E}[\\Big(\\sum\\limits_{t=0}^{T}\\log\\pi_\\theta(a_t | s_t)\\Big)\\Big(\\sum\\limits_{t'=0}^{T} \\gamma^{t'} r_{t'}\\Big)].$$\n",
        "The REINFORCE objective with temporal structure is\n",
        "$$V(\\pi_\\theta) = \\mathbb{E}[\\sum\\limits_{t=0}^{T}\\log\\pi_\\theta(a_t | s_t)\\Big(\\sum\\limits_{t'=t}^{T} \\gamma^{t'} r_{t'}\\Big)].$$\n",
        "\n",
        "The REINFORCE objective with temporal structure and baseline is\n",
        "$$V(\\pi_\\theta) = \\mathbb{E}[\\sum\\limits_{t=0}^{T}\\log\\pi_\\theta(a_t | s_t)\\Big(\\sum\\limits_{t'=t}^{T} \\gamma^{t'} r_{t'}-\\gamma^t b(s_t)\\Big)],$$\n",
        "where $b(s_t)$ is the baseline."
      ],
      "metadata": {
        "id": "2wRkQPsKU7el"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy and Value Network"
      ],
      "metadata": {
        "id": "ZVkYj2w-WqD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fill out the Network class below for the policy and value functions. PGNetwork outputs a Categorical distribution over actions, and ValueNetwork outputs (estimated) value given the current observation. You can experiment with different architectures. It is strongly recommended to set \"layer\" (the number of hidden layers) as a parameter for PGNetwork, which will be quite useful in your subsequent experiments.**"
      ],
      "metadata": {
        "id": "IvjTCAonTcUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PGNetwork(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hidden_dim = 64, layer = 2):\n",
        "        \"\"\"\n",
        "        Initialize the parameter for the policy network\n",
        "        \"\"\"\n",
        "        super(PGNetwork, self).__init__()\n",
        "        # Note: here we are given the in_dim and out_dim to the network\n",
        "        #### Your code here\n",
        "    \n",
        "\n",
        "    def forward(self, observation):\n",
        "        \"\"\"\n",
        "        This function takes in a batch of observations and a batch of actions, and \n",
        "        computes a probability distribution (Categorical) over all (discrete) actions\n",
        "        \n",
        "        observation: shape (batch_size, observation_size) torch Tensor\n",
        "        \n",
        "        return: a categorical distribution over all possible actions. You may find torch.distributions.Categorical useful\n",
        "        \"\"\"\n",
        "        #### Your code here\n",
        "\n",
        "\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim = 64):\n",
        "        \"\"\"\n",
        "        Initialize the parameter for the value function\n",
        "        \"\"\"\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        #### Your code here\n",
        "        \n",
        "\n",
        "    def forward(self, observation):\n",
        "        \"\"\"\n",
        "        This function takes in a batch of observations, and \n",
        "        computes the corresponding batch of values V(s)\n",
        "        \n",
        "        observation: shape (batch_size, observation_size) torch Tensor\n",
        "        \n",
        "        return: shape (batch_size,) values, i.e. V(observation)\n",
        "        \"\"\"\n",
        "        #### Your code here\n"
      ],
      "metadata": {
        "id": "l4g7HDm202aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trajectory Rollout\n",
        "\n",
        "**Fill out the following cells whenever you see `#### Your code here`.** Specifically, you need to complete the following functions\n",
        "- `discounted_returns`, where you compute the cumulative discounted returns from the immediate rewards\n",
        "- `update_parameters`, where you define the loss function and perform gradient update on the data collected from `rollout`."
      ],
      "metadata": {
        "id": "-Buh5AoBUXGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### You do not need to change this cell \n",
        "def rollout(model, vmodel=None, device=None, MAX_T=10000, task=task):\n",
        "    actions = torch.zeros(MAX_T, device=device, dtype=torch.int)\n",
        "    rewards = torch.zeros(MAX_T, device=device)\n",
        "    log_probs = torch.zeros(MAX_T, device=device)\n",
        "    values = torch.zeros(MAX_T, device=device)\n",
        "    obs = env.reset()\n",
        "    T = 0\n",
        "    ep_reward = 0\n",
        "    while T < MAX_T:\n",
        "        x = torch.tensor(obs, device=device, dtype=torch.float).unsqueeze(dim=0)\n",
        "        dist = model(x)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action)\n",
        "        if vmodel:\n",
        "            value = vmodel(x)\n",
        "            values[T] = value\n",
        "\n",
        "        next_obs, reward, done, _ = env.step(action.item())\n",
        "        ep_reward += reward\n",
        "        rewards[T] = reward\n",
        "        actions[T] = action\n",
        "        log_probs[T] = log_prob\n",
        "        obs = next_obs\n",
        "        \n",
        "        T += 1\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    return actions[:T], rewards[:T], log_probs[:T], values[:T], ep_reward"
      ],
      "metadata": {
        "id": "G36ZIkDa1T1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Compute the cumulative discounted return with discount factor gamma from the immediate rewards\n",
        "def discounted_returns(rewards, gamma, device=None):\n",
        "    returns = torch.zeros_like(rewards, device=device)\n",
        "    #### Your code here\n",
        "    \n",
        "    return returns\n",
        "\n",
        "\n",
        "#### Compute the policy loss from the returns and the log_probs\n",
        "def update_parameters(optimizer, model, rewards, log_probs, gamma, \n",
        "                      values=None, temporal=False, device=None):\n",
        "    # compute policy losses\n",
        "    policy_loss = []\n",
        "    returns = discounted_returns(rewards, gamma, device)\n",
        "    eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "    if values != None:\n",
        "        #### Your code here: compute value loss by fitting values \n",
        "        #### to the returns with F.smooth_l1_loss\n",
        "        value_loss = None\n",
        "    \n",
        "    if values != None:  # use the value function as the baseline\n",
        "        returns = returns - values.detach()  # this is the \"advantage\"\n",
        "        \n",
        "    #### Your code here: compute policy loss based on different objectives\n",
        "    if temporal:\n",
        "        policy_loss = None\n",
        "\n",
        "    if values != None:\n",
        "        loss = policy_loss + value_loss\n",
        "    else:\n",
        "        loss = policy_loss\n",
        "    \n",
        "    # parameter update\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "qmp_tHS41Wyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments\n",
        "\n",
        "Now we are ready to do the experiments. The following cell set up hyperparameters and construct two functions. `train()` trains the model. `visual()` visualizes the trained policy by creating a test environment and roll out a trajectory.\n",
        "\n",
        "Note: You can play around with other configuration hyperparameters if you want."
      ],
      "metadata": {
        "id": "DPHPchY1XPYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### hyperparameters\n",
        "device = torch.device('cpu') # change to 'cuda' when ready to run on GPU\n",
        "gamma = 0.999\n",
        "LR = 1e-3\n",
        "task = 'CartPole-v1'\n",
        "env = gym.make(task)\n",
        "layer = 0\n",
        "MAX_EPISODES = 1001\n",
        "LOG_INTERVAL = 100\n",
        "STEP_SAVE = 100 # Save the model in every STEP_SAVE steps\n",
        "USE_VALUE_NETWORK = False\n",
        "TEMPORAL = False\n",
        "\n",
        "\n",
        "def train():\n",
        "    #### Train the model\n",
        "    model = PGNetwork(env.observation_space.shape[0], env.action_space.n, 64, layer).to(device)\n",
        "    if USE_VALUE_NETWORK:\n",
        "        vmodel = ValueNetwork(env.observation_space.shape[0], 64).to(device)\n",
        "        optimizer = optim.Adam(list(model.parameters()) + list(vmodel.parameters()), lr=LR)\n",
        "    else:\n",
        "        optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    running_reward = 0\n",
        "    history_reward = [] # store moving average of empirical rewards\n",
        "\n",
        "    for step in trange(MAX_EPISODES):\n",
        "        actions, rewards, log_probs, values, ep_reward = rollout(\n",
        "            model, vmodel=vmodel if USE_VALUE_NETWORK else None, device=device, task=task)   \n",
        "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "        history_reward.append(running_reward)\n",
        "        update_parameters(optimizer, model, rewards, log_probs, gamma, \n",
        "                          values=values if USE_VALUE_NETWORK else None, \n",
        "                          temporal=TEMPORAL, device=device)\n",
        "        \n",
        "        if step % LOG_INTERVAL == 0:\n",
        "            print('Episode {}\\tLast reward: {:.2f} \\tAverage reward: {:.2f}'.format(\n",
        "                  step, ep_reward, running_reward))\n",
        "        \n",
        "        if step % STEP_SAVE == 0:\n",
        "            # Saves model checkpoint\n",
        "            save_checkpoint(model, optimizer, step, save_dir=f'{task}_pg'+str(layer))\n",
        "        \n",
        "    save_checkpoint(model, optimizer, step, save_dir=f'{task}_pg'+str(layer))\n",
        "    return history_reward\n",
        "\n",
        "\n",
        "def visual(step='max'):\n",
        "    #### visualize the learned policy by rolling out a trajectory\n",
        "\n",
        "    task = 'CartPole-v1'\n",
        "    video_dir = f'{task}_pg_vis'  # change the visualize direction if needed\n",
        "    env_test = wrap_env(gym.make(task), video_dir)\n",
        "\n",
        "    model = PGNetwork(env_test.observation_space.shape[0], env_test.action_space.n, 64, layer)  \n",
        "    # change save_dir to the corresponding directory\n",
        "    # You can adjust step\n",
        "    _ = load_checkpoint(model, step=step, save_dir=f'{task}_pg'+str(layer))\n",
        "\n",
        "    obs = env_test.reset()\n",
        "\n",
        "    MAX_T = 5000\n",
        "    sum_reward = 0\n",
        "    for t in range(MAX_T):\n",
        "        env.render()\n",
        "        \n",
        "        dist = model(torch.tensor(obs, dtype=torch.float).unsqueeze(dim=0))\n",
        "        action = dist.logits.argmax().numpy()\n",
        "        next_obs, reward, done, info = env_test.step(action)\n",
        "        obs = next_obs\n",
        "        sum_reward += reward\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print('%s steps' % t)\n",
        "    print('%s reward' % sum_reward)\n",
        "    env_test.close()\n",
        "    show_video(video_dir)"
      ],
      "metadata": {
        "id": "7uD1KwWk2Bb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanilla v.s. Temporal"
      ],
      "metadata": {
        "id": "vEEo4QSlS4Y7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(a): Print out the training process and the visualization results. Compare the performance between the two methods. What do you observe regarding the convergence and the rewards?**"
      ],
      "metadata": {
        "id": "wyD8z7IdOkWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Enter you answer here.*"
      ],
      "metadata": {
        "id": "-Vm93__8S1hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Your code here\n",
        "layer = 2\n",
        "LR = 1e-3\n",
        "MAX_EPISODES = 1001\n"
      ],
      "metadata": {
        "id": "e677DHBn2EN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## REINFORCE with Baseline"
      ],
      "metadata": {
        "id": "VBUauTeuUDs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(b): In the following, we always use the temporal structure. Try using REINFORCE with or without the baseline. Print out the training process and the visualization results. Plot the moving average of the rewards during the training process in a single figure (This is the output of train(). You can try multiple runs and take the mean). How does adding a baseline affect the convergence as well as the robustness of the training procedure?**"
      ],
      "metadata": {
        "id": "Df2cPaewVX11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Enter you answer here.*"
      ],
      "metadata": {
        "id": "dhvV6WCRZ7zP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters\n",
        "TEMPORAL = True\n",
        "layer = 2\n",
        "LR = 1e-3\n",
        "MAX_EPISODES = 1001"
      ],
      "metadata": {
        "id": "MHjeewxEUPRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Your code here: Run the test\n"
      ],
      "metadata": {
        "id": "_AYCOkbrJF7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### You code here: Plot the moving average rewards during the training process\n"
      ],
      "metadata": {
        "id": "YXM_1OLJYxbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hand-designed feature: Will it work?"
      ],
      "metadata": {
        "id": "O3WB4G63UyTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the OpenAI gym [leaderboard](https://github.com/openai/gym/wiki/Leaderboard#cartpole-v0), Zhiqing claimed to solve CartPole-v0 using an ingenious hand-designed policy. In particular, the policy is based on only the angle and the angular velocity (why this could work): if $3\\theta + \\dot\\theta > 0$, take action $1$; otherwise take action $0$. Your task is to first implement this policy in the first cell, and then to run the second cell to visualize the results.\n",
        "\n",
        "**(c): Describe the quality of this policy and discuss its potential limitations.**"
      ],
      "metadata": {
        "id": "9j7GG3wXbkJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Enter your answer here.*"
      ],
      "metadata": {
        "id": "MAQUM1CMcTYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hand designed policy\n",
        "def policy_naive(obs):\n",
        "    #### Your code here\n",
        "    return None"
      ],
      "metadata": {
        "id": "wGb0U_-u_Dts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### visualize the learned policy by rolling out a trajectory\n",
        "task = 'CartPole-v1'\n",
        "video_dir = f'{task}_pg_vis'  # change the visualize direction if needed\n",
        "env_test = wrap_env(gym.make(task), video_dir)\n",
        "\n",
        "obs = env_test.reset()\n",
        "\n",
        "MAX_T = 5000\n",
        "sum_reward = 0\n",
        "for t in range(MAX_T):\n",
        "    env.render()\n",
        "    \n",
        "    action = policy_naive(obs)\n",
        "    next_obs, reward, done, info = env_test.step(action)\n",
        "    obs = next_obs\n",
        "    sum_reward += reward\n",
        "    \n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print('%s steps' % t)\n",
        "print('%s reward' % sum_reward)\n",
        "env_test.close()\n",
        "show_video(video_dir)"
      ],
      "metadata": {
        "id": "hwsaFnNt_EJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might be surprised at the performance of such a simple hand-designed policy. Such a policy can be viewed as a classic [PID controller](https://en.wikipedia.org/wiki/PID_controller) that makes the angle $\\theta$ track $0$. Specifically, the policy is a thresholding PD controller. Now let us go one step further to gain some intuition about the performance of such a linear controller, and see if we can learn, by policy gradient, a policy that incorporates our intuition.\n",
        "\n",
        "Consider an approximation of the system if the control $a$ is continuous and $\\theta, \\dot \\theta$ are close to 0. Then, $\\sin \\theta \\approx \\theta$, $\\cos \\theta \\approx 1$ and second-order terms $\\theta^2, \\dot \\theta^2$ can be neglected if they appear together with constants or first-order terms $\\theta, \\dot \\theta$. The transition ([source code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py); [physical equations](https://coneural.org/florian/papers/05_cart_pole.pdf)), using $(x, \\dot x, \\theta, \\dot \\theta)$ as a state, is a linear dynamical system. To maximize the total reward, we want the cartpole to stay as vertical as possible; hence, we can redefine a cost function $c_1 \\|\\theta\\|^2 + c_2 \\|a\\|^2$ (more generally, the first term is a quadratic function in $(x, \\dot x, \\theta, \\dot \\theta)$, if we want to further encourage it to stay around the center or to have low velocity). This falls into the LQ regime, to which the optimal solution is known to be a linear controller.\n",
        "\n",
        "Since the system is indeed initialized at small $\\theta, \\dot \\theta$, it makes sense to consider linear controllers like $\\mu(s) = A s$. However, our action space is discrete in this problem, so we need to find a way to map the continuous actions to discrete ones, and such a mapping should be differentiable if we want to use policy gradient. One choice is to convert the value of the continuous action to a probability by the sigmoid function. Alternatively, you can use a $2 \\times 4$ dimensional matrix $A$ to predict a $2$ dimensional vector, and then treat the components as logits (that is, to use softmax) to generate a discrete action.\n",
        "\n",
        "**(d): Our intuition suggests such a \"linear\" policy could be reasonable. Now it's your turn to implement it, and to train it by REINFORCE. Specifically, you can play with different learning rates and different number of training episodes. What is the appropriate learning rate from your experiments? Report the linear coefficients from your policy and compare them with those from the hand-designed policy ($[0\\ 0\\ 3\\ 1]^\\top$). How do the number of parameters and the performance of your policy compared with those of REINFORCE with Baseline?**"
      ],
      "metadata": {
        "id": "Ip3qOzUt_NAw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*A: Enter your answer here.*"
      ],
      "metadata": {
        "id": "djsyXh-f-XdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Your code here\n",
        "\n",
        "layer = 0\n",
        "LR = 0\n",
        "MAX_EPISODES = 0\n",
        "USE_VALUE_NETWORK = None\n"
      ],
      "metadata": {
        "id": "PbCOle-pKOOb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}